import express from 'express';
import cors from 'cors';
import OpenAI from 'openai';
import dotenv from 'dotenv';

// ConfiguraÃ§Ã£o inicial: Carrega as variÃ¡veis do ficheiro .env
dotenv.config();

const app = express();
const PORT = process.env.PORT || 3001;

// Middlewares essenciais para o funcionamento da API
app.use(cors()); // Permite que o frontend (Vite) aceda a esta API
app.use(express.json()); // Permite receber e ler dados em formato JSON

// ConfiguraÃ§Ã£o do Cliente OpenAI adaptado para o OpenRouter
// O OpenRouter segue o mesmo protocolo da OpenAI, permitindo usar modelos gratuitos
const openai = new OpenAI({
    apiKey: process.env.VITE_OPENAI_API_KEY,
    baseURL: 'https://openrouter.ai/api/v1',
    defaultHeaders: {
        'HTTP-Referer': 'http://localhost:5173', // ObrigatÃ³rio para o OpenRouter
        'X-Title': 'EcoAgent',                  // IdentificaÃ§Ã£o opcional do projeto
    }
});

// Prompt de Sistema: Define a personalidade e regras do EcoAgent
const SYSTEM_PROMPT = `VocÃª Ã© o EcoAgent, um assistente especializado em energia, alimentado pelo modelo gpt-oss-120b via OpenRouter.
Sua missÃ£o Ã© analisar dados de consumo e sugerir otimizaÃ§Ãµes.
Sempre que o usuÃ¡rio fornecer dados, procure por anomalias (picos acima de 20% da mÃ©dia).
Seja conciso e use formataÃ§Ã£o Markdown para listas e Ãªnfase.

Diretrizes de Personalidade:
- Tom: Profissional, prestativo e analÃ­tico.
- Objetivo: Ajudar o usuÃ¡rio a reduzir custos e entender seu consumo.
- LimitaÃ§Ãµes: NÃ£o deve dar conselhos financeiros legais, apenas estimativas baseadas em dados tÃ©cnicos.

Ferramentas disponÃ­veis:
1. analyzeEnergyConsumption: Analisa dados de consumo e identifica picos e anomalias
2. suggestImprovements: Sugere aÃ§Ãµes para melhorar eficiÃªncia energÃ©tica`;

// DefiniÃ§Ã£o tÃ©cnica das ferramentas que o modelo pode decidir "chamar"
const tools = [
    {
        type: 'function',
        function: {
            name: 'analyzeEnergyConsumption',
            description: 'Analisa dados de consumo e identifica picos e anomalias',
            parameters: {
                type: 'object',
                properties: {
                    period: { type: 'string', description: 'O perÃ­odo a analisar (ex: "Ãºltima semana")' },
                },
            },
        },
    },
    {
        type: 'function',
        function: {
            name: 'suggestImprovements',
            description: 'Sugere aÃ§Ãµes para melhorar eficiÃªncia energÃ©tica baseada no contexto',
            parameters: {
                type: 'object',
                properties: {
                    category: { type: 'string', enum: ['iluminaÃ§Ã£o', 'climatizaÃ§Ã£o', 'maquinÃ¡rio', 'geral'] },
                },
            },
        },
    }
];

// ImplementaÃ§Ã£o real (simulada) das ferramentas
const toolsImplementation = {
    analyzeEnergyConsumption: (args) => {
        console.log('ðŸ› ï¸ Executando tool: analyzeEnergyConsumption', args);
        const period = args.period || 'Ãºltimo mÃªs';
        // Simulando uma anÃ¡lise de dados real
        return {
            status: 'success',
            period: period,
            summary: 'Detectado um pico de consumo de 25% na terÃ§a-feira entre as 18h e 20h.',
            recommendation: 'Verificar sistema de climatizaÃ§Ã£o neste horÃ¡rio.',
            anomalies: [
                { time: 'TerÃ§a, 18:30', value: '4.5kWh', average: '3.2kWh' }
            ]
        };
    },
    suggestImprovements: (args) => {
        console.log('ðŸ› ï¸ Executando tool: suggestImprovements', args);
        const category = args.category || 'geral';
        const suggestions = {
            'iluminaÃ§Ã£o': ['Trocar lÃ¢mpadas fluorescentes por LED', 'Instalar sensores de presenÃ§a'],
            'climatizaÃ§Ã£o': ['Limpar filtros do AC', 'Ajustar temperatura para 23Â°C constante'],
            'maquinÃ¡rio': ['Realizar manutenÃ§Ã£o preventiva nos motores', 'Trocar correias gastas'],
            'geral': ['Instalar painÃ©is solares', 'Negociar nova tarifa horÃ¡ria']
        };
        return {
            category: category,
            recommendations: suggestions[category] || suggestions['geral']
        };
    }
};

// Rota Principal de Chat: Suporta Streaming e Tool Calling (Agente)
app.post('/api/chat', async (req, res) => {
    try {
        const { messages } = req.body;

        if (!messages || !Array.isArray(messages)) {
            return res.status(400).json({ error: 'Lista de mensagens Ã© obrigatÃ³ria' });
        }

        // Define os headers de streaming imediatamente para suportar a resposta em tempo real
        res.setHeader('Content-Type', 'text/plain; charset=utf-8');
        res.setHeader('Cache-Control', 'no-cache');
        res.setHeader('Connection', 'keep-alive');

        let currentMessages = [
            { role: 'system', content: SYSTEM_PROMPT },
            ...messages,
        ];

        // Loop de execuÃ§Ã£o do Agente (permite que ele chame ferramentas e use o resultado)
        let iterations = 0;
        const maxIterations = 3;

        while (iterations < maxIterations) {
            iterations++;

            const completion = await openai.chat.completions.create({
                model: 'openai/gpt-3.5-turbo',
                messages: currentMessages,
                tools: tools,
                tool_choice: 'auto',
                temperature: 0.7,
                stream: false, // Para gerir tool calling no backend de forma simples, usamos stream: false na primeira fase
            });

            const message = completion.choices[0].message;
            currentMessages.push(message);

            // Se o modelo decidir NÃƒO chamar ferramentas, enviamos o conteÃºdo final e paramos
            if (!message.tool_calls) {
                if (message.content) {
                    res.write(message.content);
                }
                break;
            }

            // Se o modelo decidir CHAMAR ferramentas
            for (const toolCall of message.tool_calls) {
                const functionName = toolCall.function.name;
                const functionArgs = JSON.parse(toolCall.function.arguments);

                // Envia uma notificaÃ§Ã£o visual via stream (opcional, melhora UX)
                res.write(`\n> [Agente executando: ${functionName}...]\n\n`);

                const functionResponse = toolsImplementation[functionName](functionArgs);

                currentMessages.push({
                    tool_call_id: toolCall.id,
                    role: 'tool',
                    name: functionName,
                    content: JSON.stringify(functionResponse),
                });
            }
            // O loop continua para que o modelo analise o resultado da ferramenta e responda ao usuÃ¡rio
        }

        res.end();
    } catch (error) {
        console.error('âŒ Erro na API:', error);
        if (!res.headersSent) {
            res.status(500).json({ error: 'Erro de conexÃ£o com a IA' });
        } else {
            res.write('\n\n[Erro: Ocorreu uma interrupÃ§Ã£o na conexÃ£o com a IA]');
            res.end();
        }
    }
});

// Endpoint de Chat Simples (Fallback) para casos onde o streaming nÃ£o Ã© desejado
app.post('/api/chat/simple', async (req, res) => {
    try {
        const { messages } = req.body;

        if (!messages || !Array.isArray(messages)) {
            return res.status(400).json({ error: 'Lista de mensagens Ã© obrigatÃ³ria' });
        }

        const messagesWithSystem = [
            { role: 'system', content: SYSTEM_PROMPT },
            ...messages,
        ];

        const completion = await openai.chat.completions.create({
            model: 'meta-llama/llama-3.3-70b-instruct:free',
            messages: messagesWithSystem,
            temperature: 0.7,
            max_tokens: 1000,
        });

        const response = completion.choices[0]?.message?.content || '';
        res.json({ content: response });
    } catch (error) {
        console.error('Erro no endpoint de chat simples:', error);
        res.status(500).json({
            error: 'Erro interno no servidor',
            message: error.message
        });
    }
});

// Endpoint de monitorizaÃ§Ã£o (Health Check)
app.get('/api/health', (req, res) => {
    res.json({ status: 'ok', server: 'EcoAgent API', timestamp: new Date().toISOString() });
});

// Inicia o servidor na porta configurada
app.listen(PORT, () => {
    console.log(`ðŸš€ EcoAgent API Server running on http://localhost:${PORT}`);
    console.log(`ðŸ“¡ Chat endpoint: http://localhost:${PORT}/api/chat`);
    console.log(`ðŸ“¡ Model: meta-llama/llama-3.3-70b-instruct:free`);
});
