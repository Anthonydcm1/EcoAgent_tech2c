import express from 'express';
import cors from 'cors';
import OpenAI from 'openai';
import dotenv from 'dotenv';

// ConfiguraÃ§Ã£o inicial: Carrega as variÃ¡veis do ficheiro .env
dotenv.config();

const app = express();
const PORT = process.env.PORT || 3001;

// Middlewares essenciais para o funcionamento da API
app.use(cors()); // Permite que o frontend (Vite) aceda a esta API
app.use(express.json()); // Permite receber e ler dados em formato JSON

// ConfiguraÃ§Ã£o do Cliente OpenAI adaptado para o OpenRouter
// O OpenRouter segue o mesmo protocolo da OpenAI, permitindo usar modelos gratuitos
const openai = new OpenAI({
    apiKey: process.env.VITE_OPENAI_API_KEY,
    baseURL: 'https://openrouter.ai/api/v1',
    defaultHeaders: {
        'HTTP-Referer': 'http://localhost:5173', // ObrigatÃ³rio para o OpenRouter
        'X-Title': 'EcoAgent',                  // IdentificaÃ§Ã£o opcional do projeto
    }
});

// Prompt de Sistema: Define a personalidade e regras do EcoAgent
const SYSTEM_PROMPT = `VocÃª Ã© o EcoAgent, um assistente especializado em energia, alimentado pelo modelo gpt-oss-120b via OpenRouter.
Sua missÃ£o Ã© analisar dados de consumo e sugerir otimizaÃ§Ãµes.
Sempre que o usuÃ¡rio fornecer dados, procure por anomalias (picos acima de 20% da mÃ©dia).
Seja conciso e use formataÃ§Ã£o Markdown para listas e Ãªnfase.

Diretrizes de Personalidade:
- Tom: Profissional, prestativo e analÃ­tico.
- Objetivo: Ajudar o usuÃ¡rio a reduzir custos e entender seu consumo.
- LimitaÃ§Ãµes: NÃ£o deve dar conselhos financeiros legais, apenas estimativas baseadas em dados tÃ©cnicos.

Ferramentas disponÃ­veis:
1. analyzeEnergyConsumption: Analisa dados de consumo e identifica picos e anomalias
2. suggestImprovements: Sugere aÃ§Ãµes para melhorar eficiÃªncia energÃ©tica`;

// DefiniÃ§Ã£o tÃ©cnica das ferramentas que o modelo pode decidir "chamar"
const tools = [
    {
        type: 'function',
        function: {
            name: 'analyzeEnergyConsumption',
            description: 'Analisa dados de consumo e identifica picos e anomalias',
            parameters: {
                type: 'object',
                properties: {
                    period: { type: 'string', description: 'O perÃ­odo a analisar (ex: "Ãºltima semana")' },
                },
            },
        },
    },
    {
        type: 'function',
        function: {
            name: 'suggestImprovements',
            description: 'Sugere aÃ§Ãµes para melhorar eficiÃªncia energÃ©tica baseada no contexto',
            parameters: {
                type: 'object',
                properties: {
                    category: { type: 'string', enum: ['iluminaÃ§Ã£o', 'climatizaÃ§Ã£o', 'maquinÃ¡rio', 'geral'] },
                },
            },
        },
    }
];

// Rota Principal de Chat: Suporta Streaming (respostas em tempo real)
app.post('/api/chat', async (req, res) => {
    try {
        const { messages } = req.body;

        if (!messages || !Array.isArray(messages)) {
            return res.status(400).json({ error: 'Lista de mensagens Ã© obrigatÃ³ria' });
        }

        // Combina o prompt de sistema com o histÃ³rico do utilizador
        const messagesWithSystem = [
            { role: 'system', content: SYSTEM_PROMPT },
            ...messages,
        ];

        // 1. Solicita a resposta ao modelo ANTES de definir os headers
        const stream = await openai.chat.completions.create({
            model: 'openai/gpt-3.5-turbo', // Modelo oficial OpenAI via OpenRouter
            messages: messagesWithSystem,
            stream: true,
            temperature: 0.7,
            max_tokens: 1000,
        });

        // 2. Se chegou aqui, a ligaÃ§Ã£o funcionou. Define os headers de streaming.
        res.setHeader('Content-Type', 'text/plain; charset=utf-8');
        res.setHeader('Cache-Control', 'no-cache');
        res.setHeader('Connection', 'keep-alive');

        // 3. Envia os dados como texto puro (compatÃ­vel com useChat)
        for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            if (content) {
                res.write(content);
            }
        }

        res.end();
    } catch (error) {
        console.error('âŒ Erro na API:', error.message);

        // Se ainda nÃ£o enviamos nada, podemos enviar um JSON de erro
        if (!res.headersSent) {
            if (error.status === 404 || error.message?.includes('data policy')) {
                return res.status(500).json({
                    error: 'ConfiguraÃ§Ã£o do OpenRouter necessÃ¡ria',
                    message: 'Verifique se ativou o 3Âº interruptor no OpenRouter e REINICIE o comando npm run dev:full.'
                });
            }
            return res.status(500).json({
                error: 'Erro de conexÃ£o com a IA',
                message: error.message || 'O servidor de IA nÃ£o respondeu corretamente.'
            });
        }
        res.end();
    }
});

// Endpoint de Chat Simples (Fallback) para casos onde o streaming nÃ£o Ã© desejado
app.post('/api/chat/simple', async (req, res) => {
    try {
        const { messages } = req.body;

        if (!messages || !Array.isArray(messages)) {
            return res.status(400).json({ error: 'Lista de mensagens Ã© obrigatÃ³ria' });
        }

        const messagesWithSystem = [
            { role: 'system', content: SYSTEM_PROMPT },
            ...messages,
        ];

        const completion = await openai.chat.completions.create({
            model: 'meta-llama/llama-3.3-70b-instruct:free',
            messages: messagesWithSystem,
            temperature: 0.7,
            max_tokens: 1000,
        });

        const response = completion.choices[0]?.message?.content || '';
        res.json({ content: response });
    } catch (error) {
        console.error('Erro no endpoint de chat simples:', error);
        res.status(500).json({
            error: 'Erro interno no servidor',
            message: error.message
        });
    }
});

// Endpoint de monitorizaÃ§Ã£o (Health Check)
app.get('/api/health', (req, res) => {
    res.json({ status: 'ok', server: 'EcoAgent API', timestamp: new Date().toISOString() });
});

// Inicia o servidor na porta configurada
app.listen(PORT, () => {
    console.log(`ðŸš€ EcoAgent API Server running on http://localhost:${PORT}`);
    console.log(`ðŸ“¡ Chat endpoint: http://localhost:${PORT}/api/chat`);
    console.log(`ðŸ“¡ Model: meta-llama/llama-3.3-70b-instruct:free`);
});
